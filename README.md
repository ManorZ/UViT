# UViT
Unstructured Vision Transformer.
Replacing image patches as token by unstructured interest points axtracting algorithm.

06/05/22  
1. Let's start from here: https://huggingface.co/docs/transformers/model_doc/vit __
Looks like it contains resources for pre-trained ViT in PyTorch + some example notebooks __
2. Original Google's GitHub: https://github.com/google-research/vision_transformer __
it is written in Jax, which I'm not familar with, so I think we should prefer using Pytorch-based model, at least for start __
